{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "774eb43d-d98d-4348-aa56-134fd35d7135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Treating categorical feature 2 with 5 values as binary.\n",
      "Warning: Treating categorical feature 3 with 4 values as binary.\n",
      "Warning: Treating categorical feature 3 with 3 values as binary.\n",
      "Accuracy with pre-discretization: 0.9556\n",
      "Accuracy with integrated continuous feature handling: 0.9778\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from math import log2\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
    "        self.feature = feature      # Feature index to split on\n",
    "        self.threshold = threshold  # Threshold for continuous features\n",
    "        self.left = left            # Left subtree (<=)\n",
    "        self.right = right          # Right subtree (>)\n",
    "        self.value = value          # Leaf node prediction value\n",
    "\n",
    "class ID3DecisionTree:\n",
    "    def __init__(self, max_depth=None, min_samples_split=2, min_info_gain=0.01, discretize_method='equal_width', n_bins=5):\n",
    "        \"\"\"\n",
    "        ID3 Decision Tree that can handle continuous features through discretization.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        max_depth : int or None\n",
    "            Maximum depth of the tree\n",
    "        min_samples_split : int\n",
    "            Minimum number of samples required to split a node\n",
    "        min_info_gain : float\n",
    "            Minimum information gain required to split a node\n",
    "        discretize_method : str\n",
    "            Method to discretize continuous variables ('equal_width', 'equal_frequency')\n",
    "        n_bins : int\n",
    "            Number of bins to use when discretizing continuous variables\n",
    "        \"\"\"\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_info_gain = min_info_gain\n",
    "        self.discretize_method = discretize_method\n",
    "        self.n_bins = n_bins\n",
    "        self.root = None\n",
    "        self.feature_types = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Train the decision tree.\"\"\"\n",
    "        # Identify feature types (continuous or categorical)\n",
    "        self.feature_types = self._identify_feature_types(X)\n",
    "        self.root = self._build_tree(X, y, depth=0)\n",
    "        return self\n",
    "    \n",
    "    def _identify_feature_types(self, X):\n",
    "        \"\"\"Identify if features are continuous or categorical.\"\"\"\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            feature_types = []\n",
    "            for col in X.columns:\n",
    "                if X[col].dtype.kind in 'iuf':  # integer, unsigned integer, or float\n",
    "                    if len(X[col].unique()) > 10:  # Assume it's continuous if many unique values\n",
    "                        feature_types.append('continuous')\n",
    "                    else:\n",
    "                        feature_types.append('categorical')\n",
    "                else:\n",
    "                    feature_types.append('categorical')\n",
    "        else:\n",
    "            # For numpy arrays, assume numeric columns with more than 10 unique values are continuous\n",
    "            feature_types = []\n",
    "            for i in range(X.shape[1]):\n",
    "                if len(np.unique(X[:, i])) > 10 and np.issubdtype(X[:, i].dtype, np.number):\n",
    "                    feature_types.append('continuous')\n",
    "                else:\n",
    "                    feature_types.append('categorical')\n",
    "        return feature_types\n",
    "    \n",
    "    def _entropy(self, y):\n",
    "        \"\"\"Calculate the entropy of a dataset.\"\"\"\n",
    "        counts = Counter(y)\n",
    "        proportions = [count / len(y) for count in counts.values()]\n",
    "        return -sum(p * log2(p) for p in proportions if p > 0)\n",
    "    \n",
    "    def _information_gain(self, X, y, feature_idx, threshold=None):\n",
    "        \"\"\"Calculate information gain for a split on a feature.\"\"\"\n",
    "        parent_entropy = self._entropy(y)\n",
    "        \n",
    "        # For continuous features, we need a threshold\n",
    "        if self.feature_types[feature_idx] == 'continuous':\n",
    "            # Split the dataset based on the threshold\n",
    "            left_mask = X[:, feature_idx] <= threshold\n",
    "            right_mask = ~left_mask\n",
    "            \n",
    "            # Check if the split is valid (both partitions have samples)\n",
    "            if sum(left_mask) == 0 or sum(right_mask) == 0:\n",
    "                return 0\n",
    "            \n",
    "            # Calculate entropy for both partitions\n",
    "            left_entropy = self._entropy(y[left_mask])\n",
    "            right_entropy = self._entropy(y[right_mask])\n",
    "            \n",
    "            # Calculate weighted average entropy\n",
    "            n = len(y)\n",
    "            n_left = sum(left_mask)\n",
    "            n_right = sum(right_mask)\n",
    "            weighted_entropy = (n_left / n) * left_entropy + (n_right / n) * right_entropy\n",
    "            \n",
    "            # Information gain\n",
    "            return parent_entropy - weighted_entropy\n",
    "        \n",
    "        # For categorical features\n",
    "        else:\n",
    "            # Get unique values of the feature\n",
    "            values = np.unique(X[:, feature_idx])\n",
    "            \n",
    "            # Calculate weighted average entropy\n",
    "            weighted_entropy = 0\n",
    "            n = len(y)\n",
    "            \n",
    "            for value in values:\n",
    "                mask = X[:, feature_idx] == value\n",
    "                if sum(mask) == 0:\n",
    "                    continue\n",
    "                \n",
    "                subset_entropy = self._entropy(y[mask])\n",
    "                weighted_entropy += (sum(mask) / n) * subset_entropy\n",
    "            \n",
    "            # Information gain\n",
    "            return parent_entropy - weighted_entropy\n",
    "    \n",
    "    def _find_best_split(self, X, y):\n",
    "        \"\"\"Find the best feature and threshold to split on.\"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # If not enough samples or all targets are the same, no split\n",
    "        if n_samples < self.min_samples_split or len(np.unique(y)) == 1:\n",
    "            return None, None\n",
    "        \n",
    "        best_gain = -1\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "        \n",
    "        # Loop through all features\n",
    "        for feature_idx in range(n_features):\n",
    "            if self.feature_types[feature_idx] == 'continuous':\n",
    "                # Find unique values for the feature\n",
    "                unique_values = sorted(np.unique(X[:, feature_idx]))\n",
    "                \n",
    "                # Calculate potential thresholds (midpoints between consecutive values)\n",
    "                thresholds = [(unique_values[i] + unique_values[i+1]) / 2 for i in range(len(unique_values) - 1)]\n",
    "                \n",
    "                # Find best threshold\n",
    "                for threshold in thresholds:\n",
    "                    gain = self._information_gain(X, y, feature_idx, threshold)\n",
    "                    \n",
    "                    if gain > best_gain:\n",
    "                        best_gain = gain\n",
    "                        best_feature = feature_idx\n",
    "                        best_threshold = threshold\n",
    "            \n",
    "            else:  # Categorical feature\n",
    "                gain = self._information_gain(X, y, feature_idx)\n",
    "                \n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature = feature_idx\n",
    "                    best_threshold = None\n",
    "        \n",
    "        # Only return a split if the gain is sufficient\n",
    "        if best_gain < self.min_info_gain:\n",
    "            return None, None\n",
    "        \n",
    "        return best_feature, best_threshold\n",
    "    \n",
    "    def _build_tree(self, X, y, depth):\n",
    "        \"\"\"Recursively build the decision tree.\"\"\"\n",
    "        # Convert pandas DataFrame to numpy array\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "        if isinstance(y, pd.Series):\n",
    "            y = y.values\n",
    "        \n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Check termination criteria\n",
    "        if (self.max_depth is not None and depth >= self.max_depth) or \\\n",
    "           (n_samples < self.min_samples_split) or \\\n",
    "           (len(np.unique(y)) == 1):\n",
    "            # Leaf node - return most common class\n",
    "            leaf_value = Counter(y).most_common(1)[0][0]\n",
    "            return Node(value=leaf_value)\n",
    "        \n",
    "        # Find best split\n",
    "        best_feature, best_threshold = self._find_best_split(X, y)\n",
    "        \n",
    "        # If no good split found, create a leaf node\n",
    "        if best_feature is None:\n",
    "            leaf_value = Counter(y).most_common(1)[0][0]\n",
    "            return Node(value=leaf_value)\n",
    "        \n",
    "        # Create internal node and children\n",
    "        if self.feature_types[best_feature] == 'continuous':\n",
    "            left_mask = X[:, best_feature] <= best_threshold\n",
    "            right_mask = ~left_mask\n",
    "            \n",
    "            left = self._build_tree(X[left_mask], y[left_mask], depth + 1)\n",
    "            right = self._build_tree(X[right_mask], y[right_mask], depth + 1)\n",
    "            \n",
    "            return Node(feature=best_feature, threshold=best_threshold, left=left, right=right)\n",
    "        \n",
    "        else:  # Categorical feature\n",
    "            # This is a simplification: in real ID3, we would create one branch per value\n",
    "            # For simplicity, we're treating categorical features like binary ones\n",
    "            unique_values = np.unique(X[:, best_feature])\n",
    "            if len(unique_values) > 2:\n",
    "                print(f\"Warning: Treating categorical feature {best_feature} with {len(unique_values)} values as binary.\")\n",
    "            \n",
    "            # Use the first value as the splitting criterion\n",
    "            left_mask = X[:, best_feature] == unique_values[0]\n",
    "            right_mask = ~left_mask\n",
    "            \n",
    "            left = self._build_tree(X[left_mask], y[left_mask], depth + 1)\n",
    "            right = self._build_tree(X[right_mask], y[right_mask], depth + 1)\n",
    "            \n",
    "            return Node(feature=best_feature, threshold=unique_values[0], left=left, right=right)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict target for X.\"\"\"\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "        \n",
    "        return np.array([self._predict_sample(sample) for sample in X])\n",
    "    \n",
    "    def _predict_sample(self, sample):\n",
    "        \"\"\"Predict target for a single sample.\"\"\"\n",
    "        node = self.root\n",
    "        \n",
    "        while node.value is None:\n",
    "            if self.feature_types[node.feature] == 'continuous':\n",
    "                if sample[node.feature] <= node.threshold:\n",
    "                    node = node.left\n",
    "                else:\n",
    "                    node = node.right\n",
    "            else:  # Categorical\n",
    "                if sample[node.feature] == node.threshold:\n",
    "                    node = node.left\n",
    "                else:\n",
    "                    node = node.right\n",
    "        \n",
    "        return node.value\n",
    "    \n",
    "    def discretize_continuous_feature(self, X, feature_idx):\n",
    "        \"\"\"\n",
    "        Discretize a continuous feature using the specified method.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like\n",
    "            Feature matrix\n",
    "        feature_idx : int\n",
    "            Index of the feature to discretize\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        X_discretized : array-like\n",
    "            Feature matrix with the specified feature discretized\n",
    "        bin_edges : array-like\n",
    "            The bin edges used for discretization\n",
    "        \"\"\"\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            feature_values = X.iloc[:, feature_idx].values\n",
    "        else:\n",
    "            feature_values = X[:, feature_idx]\n",
    "        \n",
    "        if self.discretize_method == 'equal_width':\n",
    "            # Equal width binning\n",
    "            min_val = np.min(feature_values)\n",
    "            max_val = np.max(feature_values)\n",
    "            bin_edges = np.linspace(min_val, max_val, self.n_bins + 1)\n",
    "        \n",
    "        elif self.discretize_method == 'equal_frequency':\n",
    "            # Equal frequency binning\n",
    "            bin_edges = np.percentile(feature_values, np.linspace(0, 100, self.n_bins + 1))\n",
    "            bin_edges[0] = min(feature_values) - 1e-10  # Ensure minimum value is included\n",
    "            bin_edges[-1] = max(feature_values) + 1e-10  # Ensure maximum value is included\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown discretization method: {self.discretize_method}\")\n",
    "        \n",
    "        # Discretize the feature\n",
    "        discretized = np.digitize(feature_values, bin_edges[1:-1])\n",
    "        \n",
    "        # Create a copy to avoid modifying the original data\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X_discretized = X.copy()\n",
    "            X_discretized.iloc[:, feature_idx] = discretized\n",
    "        else:\n",
    "            X_discretized = X.copy()\n",
    "            X_discretized[:, feature_idx] = discretized\n",
    "        \n",
    "        return X_discretized, bin_edges\n",
    "    \n",
    "    def discretize_all_continuous_features(self, X):\n",
    "        \"\"\"\n",
    "        Discretize all continuous features in the dataset.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like\n",
    "            Feature matrix\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        X_discretized : array-like\n",
    "            Feature matrix with all continuous features discretized\n",
    "        bin_edges_dict : dict\n",
    "            Dictionary mapping feature indices to their bin edges\n",
    "        \"\"\"\n",
    "        if self.feature_types is None:\n",
    "            self.feature_types = self._identify_feature_types(X)\n",
    "        \n",
    "        X_discretized = X.copy() if isinstance(X, pd.DataFrame) else X.copy()\n",
    "        bin_edges_dict = {}\n",
    "        \n",
    "        for i, feature_type in enumerate(self.feature_types):\n",
    "            if feature_type == 'continuous':\n",
    "                X_discretized, bin_edges = self.discretize_continuous_feature(X_discretized, i)\n",
    "                bin_edges_dict[i] = bin_edges\n",
    "        \n",
    "        return X_discretized, bin_edges_dict\n",
    "\n",
    "# Example usage\n",
    "def example_usage():\n",
    "    from sklearn.datasets import load_iris\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    \n",
    "    # Load dataset\n",
    "    iris = load_iris()\n",
    "    X = iris.data\n",
    "    y = iris.target\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Create and train the model\n",
    "    tree = ID3DecisionTree(max_depth=3, discretize_method='equal_width', n_bins=5)\n",
    "    \n",
    "    # Option 1: Discretize all continuous features before training\n",
    "    X_train_discretized, bin_edges = tree.discretize_all_continuous_features(X_train)\n",
    "    tree.fit(X_train_discretized, y_train)\n",
    "    \n",
    "    # Also discretize test data using the same bin edges\n",
    "    X_test_discretized = X_test.copy()\n",
    "    for feature_idx, edges in bin_edges.items():\n",
    "        X_test_discretized[:, feature_idx] = np.digitize(X_test[:, feature_idx], edges[1:-1])\n",
    "    \n",
    "    # Predict and evaluate\n",
    "    y_pred = tree.predict(X_test_discretized)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy with pre-discretization: {accuracy:.4f}\")\n",
    "    \n",
    "    # Option 2: Use the integrated handling of continuous features\n",
    "    tree2 = ID3DecisionTree(max_depth=3, min_info_gain=0.01)\n",
    "    tree2.fit(X_train, y_train)\n",
    "    y_pred2 = tree2.predict(X_test)\n",
    "    accuracy2 = accuracy_score(y_test, y_pred2)\n",
    "    print(f\"Accuracy with integrated continuous feature handling: {accuracy2:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    example_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df767112-61eb-41a2-8dd9-6391e04496f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
